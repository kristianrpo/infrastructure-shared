name: Deploy Shared Infrastructure

on:
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read
  id-token: write

env:
  TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}
  TF_BACKEND_KEY: shared/terraform.tfstate
  TF_BACKEND_REGION: ${{ secrets.AWS_REGION }}
  TF_BACKEND_DDB_TABLE: ${{ secrets.TF_BACKEND_DDB_TABLE }}

jobs:
  deploy-shared:
    name: Deploy Shared Infrastructure
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Bootstrap remote backend (S3 + DynamoDB lock)
        run: |
          set -euo pipefail
          # S3 bucket (crear si no existe)
          if aws s3api head-bucket --bucket "$TF_BACKEND_BUCKET" 2>/dev/null; then
            echo "Backend bucket OK: $TF_BACKEND_BUCKET"
          else
            echo "Creating backend bucket: $TF_BACKEND_BUCKET"
            if [ "$TF_BACKEND_REGION" = "us-east-1" ]; then
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET"
            else
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET" \
                --region "$TF_BACKEND_REGION" \
                --create-bucket-configuration LocationConstraint="$TF_BACKEND_REGION"
            fi
            aws s3api put-bucket-versioning --bucket "$TF_BACKEND_BUCKET" \
              --versioning-configuration Status=Enabled
            aws s3api put-bucket-encryption --bucket "$TF_BACKEND_BUCKET" \
              --server-side-encryption-configuration \
              '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
          fi

          # DynamoDB table (crear si no existe)
          if aws dynamodb describe-table --table-name "$TF_BACKEND_DDB_TABLE" >/dev/null 2>&1; then
            echo "Lock table OK: $TF_BACKEND_DDB_TABLE"
          else
            echo "Creating lock table: $TF_BACKEND_DDB_TABLE"
            aws dynamodb create-table \
              --table-name "$TF_BACKEND_DDB_TABLE" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST
            aws dynamodb wait table-exists --table-name "$TF_BACKEND_DDB_TABLE"
          fi

      - name: Terraform Init
        working-directory: terraform
        run: |
          terraform init -input=false -upgrade \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE"

      - name: Terraform Validate
        working-directory: terraform
        run: terraform validate || echo "Validation failed (expected with Kubernetes/Helm providers before cluster exists)"
        continue-on-error: true

      - name: Terraform Plan
        working-directory: terraform
        run: terraform plan -out=tfplan
        
      - name: Terraform Apply
        working-directory: terraform
        continue-on-error: true
        run: |
          set +e
          # Try to apply
          terraform apply -auto-approve tfplan 2>&1 | tee /tmp/apply.log
          APPLY_EXIT=${PIPESTATUS[0]}
          
          # If apply failed, check for specific errors
          if [ $APPLY_EXIT -ne 0 ]; then
            if grep -q "AlreadyExistsException.*alias/eks" /tmp/apply.log; then
              echo "KMS alias already exists error detected. Fixing state..."
              
              # Import the existing alias
              terraform import module.eks.module.kms.aws_kms_alias.this[\"cluster\"] "alias/eks/citizen-dev" || \
                echo "Import may have failed (alias might already be in state)"
              
              # Generate a new plan and apply
              echo "Generating new plan after import..."
              terraform plan -out=tfplan-retry
              
              echo "Applying with corrected state..."
              terraform apply -auto-approve tfplan-retry || {
                echo "Final apply failed"
                exit 1
              }
            elif grep -q "context deadline exceeded.*external_secrets" /tmp/apply.log || \
                 grep -q "context deadline exceeded.*helm_release" /tmp/apply.log; then
              echo "Helm timeout detected for external_secrets"
              echo "This is common in EKS and the resources are usually still being created in the background."
              echo "The infrastructure core (VPC, EKS, RabbitMQ) should be functional."
              echo "External Secrets Operator may need manual verification."
              echo ""
              echo "Continuing pipeline despite timeout..."
              # Don't exit 1, let the pipeline continue
            elif grep -q "namespaces \"monitoring\" already exists" /tmp/apply.log; then
              echo "Namespace 'monitoring' already exists. Importing to state..."
              
              # Import the existing namespace
              terraform import kubernetes_namespace.monitoring monitoring || \
                echo "Namespace may already be in state"
              
              # Generate a new plan and apply
              echo "Generating new plan after namespace import..."
              terraform plan -out=tfplan-retry-ns
              
              echo "Applying with corrected state..."
              terraform apply -auto-approve tfplan-retry-ns || {
                echo "Final apply failed"
                exit 1
              }
            else
              echo "Apply failed with non-recoverable error"
              cat /tmp/apply.log
              exit $APPLY_EXIT
            fi
          else
            echo "Apply completed successfully"
          fi
          
          set -e

      - name: Display Outputs
        working-directory: terraform
        continue-on-error: true
        run: |
          echo "=== Shared Infrastructure Deployed ==="
          echo ""
          
          # Cluster info
          if terraform output -raw cluster_name >/dev/null 2>&1; then
            echo "Cluster Name: $(terraform output -raw cluster_name)"
          else
            echo "Cluster Name: Not yet created"
          fi
          
          # RabbitMQ info (may not exist if apply partially failed)
          if terraform output -raw rabbitmq_host >/dev/null 2>&1; then
            echo "RabbitMQ Host: $(terraform output -raw rabbitmq_host)"
          else
            echo "RabbitMQ Host: Not created yet"
          fi
          
          # DynamoDB table
          if terraform output -raw rabbitmq_processed_messages_table_name >/dev/null 2>&1; then
            echo "DynamoDB Table: $(terraform output -raw rabbitmq_processed_messages_table_name)"
          else
            echo "DynamoDB Table: Not created yet"
          fi
          
          echo ""
          echo "Available outputs from terraform state:"
          terraform output || echo "No outputs available"
          echo ""
          echo "Note: If RabbitMQ output is missing, the infrastructure may still be deploying."
          echo "Check the pipeline logs for any KMS alias import messages."