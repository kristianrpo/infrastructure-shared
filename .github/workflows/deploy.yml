name: Deploy Shared Infrastructure

on:
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read
  id-token: write

env:
  TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}
  TF_BACKEND_KEY: shared/terraform.tfstate
  TF_BACKEND_REGION: ${{ secrets.AWS_REGION }}
  TF_BACKEND_DDB_TABLE: ${{ secrets.TF_BACKEND_DDB_TABLE }}

jobs:
  deploy-shared:
    name: Deploy Shared Infrastructure
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Bootstrap remote backend (S3 + DynamoDB lock)
        run: |
          set -euo pipefail
          # S3 bucket (crear si no existe)
          if aws s3api head-bucket --bucket "$TF_BACKEND_BUCKET" 2>/dev/null; then
            echo "Backend bucket OK: $TF_BACKEND_BUCKET"
          else
            echo "Creating backend bucket: $TF_BACKEND_BUCKET"
            if [ "$TF_BACKEND_REGION" = "us-east-1" ]; then
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET"
            else
              aws s3api create-bucket --bucket "$TF_BACKEND_BUCKET" \
                --region "$TF_BACKEND_REGION" \
                --create-bucket-configuration LocationConstraint="$TF_BACKEND_REGION"
            fi
            aws s3api put-bucket-versioning --bucket "$TF_BACKEND_BUCKET" \
              --versioning-configuration Status=Enabled
            aws s3api put-bucket-encryption --bucket "$TF_BACKEND_BUCKET" \
              --server-side-encryption-configuration \
              '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
          fi

          # DynamoDB table (crear si no existe)
          if aws dynamodb describe-table --table-name "$TF_BACKEND_DDB_TABLE" >/dev/null 2>&1; then
            echo "Lock table OK: $TF_BACKEND_DDB_TABLE"
          else
            echo "Creating lock table: $TF_BACKEND_DDB_TABLE"
            aws dynamodb create-table \
              --table-name "$TF_BACKEND_DDB_TABLE" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST
            aws dynamodb wait table-exists --table-name "$TF_BACKEND_DDB_TABLE"
          fi

      - name: Terraform Init
        working-directory: terraform
        run: |
          terraform init -input=false -upgrade \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE"

      - name: Clean up KMS alias from state if it exists
        working-directory: terraform
        continue-on-error: true
        run: |
          echo "Checking for orphaned KMS alias in state..."
          ALIAS_NAME="alias/eks/citizen-dev"
          
          # If alias exists in AWS, try to import it
          # This handles the case where the alias was created outside of Terraform
          if aws kms describe-alias --alias-name "$ALIAS_NAME" >/dev/null 2>&1; then
            echo "KMS alias exists in AWS: $ALIAS_NAME"
            echo "Attempting to import to sync state..."
            terraform import module.eks.module.kms.aws_kms_alias.this[\"cluster\"] "$ALIAS_NAME" 2>&1 || echo "Import completed or not needed"
          else
            echo "KMS alias does not exist in AWS"
          fi

      - name: Terraform Validate
        working-directory: terraform
        run: terraform validate || echo "Validation failed (expected with Kubernetes/Helm providers before cluster exists)"
        continue-on-error: true

      - name: Terraform Plan
        working-directory: terraform
        run: terraform plan -out=tfplan
        
      - name: Terraform Apply
        working-directory: terraform
        run: |
          # Run terraform apply and capture any KMS alias errors
          if ! terraform apply -auto-approve tfplan 2>&1 | tee /tmp/apply.log; then
            EXIT_CODE=$?
            # If error is KMS alias already exists, import and re-apply
            if grep -q "AlreadyExistsException.*alias" /tmp/apply.log; then
              echo "Warning: KMS alias already exists. Attempting to import and fix state..."
              terraform import module.eks.module.kms.aws_kms_alias.this[\"cluster\"] "alias/eks/citizen-dev" || true
              echo "Re-applying with updated state..."
              terraform apply -auto-approve || exit $EXIT_CODE
            else
              echo "Apply failed with error: $EXIT_CODE"
              exit $EXIT_CODE
            fi
          fi

      - name: Display Outputs
        working-directory: terraform
        run: |
          echo "=== Shared Infrastructure Deployed ==="
          echo "Cluster Name: $(terraform output -raw cluster_name)"
          echo "RabbitMQ Host: $(terraform output -raw rabbitmq_host)"
          echo "DynamoDB Table: $(terraform output -raw rabbitmq_processed_messages_table_name)"
          echo ""
          echo "Available Outputs:"
          echo "- VPC ID"
          echo "- EKS Cluster"
          echo "- RabbitMQ Broker"
          echo "- DynamoDB Table for Idempotency"
          echo "- IAM Roles and Policies"